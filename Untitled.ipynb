{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12d2689f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pl/wm7wjyvn1vvflk70yn1b6n6c0000gn/T/ipykernel_15990/3916065158.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Scraper functions, which in turn are sourced in runfile.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "# Scraper functions, which in turn are sourced in runfile.py\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "# Initialize browser & wait for 15 seconds\n",
    "def init_browser(filepath):\n",
    "    browser = webdriver.Chrome(executable_path=filepath)\n",
    "    browser.implicitly_wait(10)\n",
    "    return browser\n",
    "\n",
    "# Open target website\n",
    "\n",
    "def navigate_to_website(browser):\n",
    "    browser.get('https://www.funda.nl')\n",
    "\n",
    "# Wait until site elements are loaded on the home page and enter search term\n",
    "\n",
    "def enter_search_term(browser, search_term):\n",
    "\n",
    "    wait = WebDriverWait(browser, 10)\n",
    "\n",
    "    try:\n",
    "        search_bar = wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, \"//input[@id='autocomplete-input']\")))\n",
    "        button = wait.until(EC.element_to_be_clickable(\n",
    "            (By.XPATH, \"//button[@class='button-primary-alternative']\")))\n",
    "        search_bar.click()\n",
    "        time.sleep(randint(10, 15))\n",
    "        search_bar.clear()\n",
    "        time.sleep(randint(10, 15))\n",
    "        search_bar.send_keys(search_term)\n",
    "        time.sleep(randint(10, 15))\n",
    "        button.click()\n",
    "        print(\"search-button has been clicked\")\n",
    "        time.sleep(randint(15, 20))\n",
    "        return True\n",
    "    except (TimeoutException, NoSuchElementException) as e:\n",
    "        print(str(e))\n",
    "        return False\n",
    "\n",
    "# Scrape the resulting page and move on to the next page until hitting the predefined lastpage. All results are stored in a csv-file\n",
    "\n",
    "def get_data(browser, lastpage, search_term):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    keep_going = True\n",
    "    wait = WebDriverWait(browser, 15)\n",
    "    page = 2\n",
    "\n",
    "    while keep_going and page <= lastpage:\n",
    "\n",
    "        try:\n",
    "            for item in browser.find_elements_by_css_selector(\"div.search-result-content\"):\n",
    "\n",
    "                try:\n",
    "                    zipcode1, zipcode2, city = item.find_element_by_css_selector(\n",
    "                        \"small.search-result-subtitle\").text.split(\" \", 2)\n",
    "                    zipcode = zipcode1 + \" \" + zipcode2\n",
    "\n",
    "                    street_zipcode_city = item.find_element_by_css_selector(\"h3.search-result-title\").text\n",
    "\n",
    "                    price = item.find_element_by_css_selector(\"span.search-result-price\").text.lstrip('€ ').rstrip(\n",
    "                        ' k.k,').replace('.', '')\n",
    "\n",
    "                    surface, rooms = item.find_element_by_css_selector(\"ul.search-result-kenmerken\").text.replace('\\n',\n",
    "                                                                                                                  '').replace(\n",
    "                        'm²', '').split(\" \", 1)\n",
    "                    rooms = rooms.replace('kamer', '').replace('s', '')\n",
    "\n",
    "                    link = item.find_element_by_css_selector(\"div.search-result-header>a\").get_attribute('href')\n",
    "\n",
    "                    data.append({\n",
    "                        \"street_zipcode_city\": street_zipcode_city,\n",
    "                        \"zipcode\": zipcode,\n",
    "                        \"city\": city,\n",
    "                        \"price\": price,\n",
    "                        \"surface\": surface,\n",
    "                        \"rooms\": rooms,\n",
    "                        \"link\": link,\n",
    "                    })\n",
    "\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            print(\"page extracted\")\n",
    "            time.sleep(randint(5, 10))\n",
    "\n",
    "            browser.find_element_by_xpath(\"//a[contains(@href,'\" + search_term + \"') and contains(@data-pagination-page,'\" + str(page) + \"') and contains(@class, 'pagination-number')]\").click()\n",
    "            print(\"link to page \" + str(page) + \" has been clicked\")\n",
    "            page += 1\n",
    "            time.sleep(randint(5, 15))\n",
    "\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            keep_going = False\n",
    "\n",
    "    browser.close()\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(search_term+\"uptopage\"+str(lastpage)+\".csv\", sep=';', encoding='utf-8')\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52401a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
